{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da4158ef",
   "metadata": {},
   "source": [
    "# Classification of Images of Geometric Shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b730402",
   "metadata": {},
   "source": [
    "## 1. Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "223200ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image as mpimg\n",
    "import cv2\n",
    "\n",
    "from keras.utils import to_categorical, load_img\n",
    "from keras.layers import Dense,Conv2D,Flatten,MaxPool2D,Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f36c679",
   "metadata": {},
   "source": [
    "## 2. Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79fdd636",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_direc = 'geometric shapes dataset'\n",
    "classes = []\n",
    "#Obtain the name of the class directories we will work with\n",
    "for i in os.listdir(data_direc):\n",
    "    classes.append(i)\n",
    "\n",
    "#read in images from separate class directories\n",
    "data = []\n",
    "class_labels = []\n",
    "scale_factor = 0.5\n",
    "\n",
    "label = 0\n",
    "for i in classes:\n",
    "    path = data_direc+'/'+str(i)\n",
    "    images = os.listdir(path)\n",
    "    for img in images:\n",
    "        image = cv2.imread(path+'/'+img,0) #read in the image in greyscale to reduce computation time\n",
    "        size = (int(image.shape[1] * scale_factor), int(image.shape[0] * scale_factor)) \n",
    "        image = cv2.resize(image, size, interpolation=cv2.INTER_AREA) #resize using aspect ratio\n",
    "        image = np.array(image)\n",
    "        data.append(image) #add image to dataset\n",
    "        class_labels.append(label) #add associated integer label to class_labels list\n",
    "    label = label + 1\n",
    "\n",
    "plt.imshow(data[29999], cmap='gray')\n",
    "print(data[100].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6714534d",
   "metadata": {},
   "source": [
    "## 3. Split data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35da7eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500, 100, 100, 1)\n",
      "(7500, 100, 100, 1)\n",
      "(22500, 3)\n",
      "(7500, 3)\n"
     ]
    }
   ],
   "source": [
    "#Convert images and labels to numpy arrays\n",
    "training_images = np.array(data)\n",
    "training_labels = np.array(class_labels)\n",
    "\n",
    "#Convert labels into One-Hot Encoded labels\n",
    "training_labels = to_categorical(training_labels, num_classes = 3)\n",
    "\n",
    "#Split into training and testing data\n",
    "x_train, x_test, y_train, y_test = train_test_split(training_images, training_labels,   \n",
    "                                                    test_size = 0.25, random_state = 42, shuffle=True)\n",
    "\n",
    "#Regularize images to ease computation\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "\n",
    "#Reshape, adding a channels dimension\n",
    "x_train = x_train.reshape(-1,100,100,1)\n",
    "x_test = x_test.reshape(-1,100,100,1)\n",
    "\n",
    "#Verify dimensions of training and testing sets\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a62be85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add data augmentation to prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71e3352",
   "metadata": {},
   "source": [
    "## 4. Convolutional Neural Network Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acb5f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: add dropouts/l1 or l2 regularization to help prevent overfitting\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 8, kernel_size = 2, padding = \"same\", activation = \"relu\", input_shape = (100,100,1)))\n",
    "model.add(MaxPool2D(pool_size = 2))\n",
    "model.add(Conv2D(filters = 16, kernel_size = 2, padding = \"same\", activation = \"relu\"))\n",
    "model.add(MaxPool2D(pool_size = 2))\n",
    "model.add(Conv2D(filters = 32, kernel_size = 2, padding = \"same\", activation = \"relu\"))\n",
    "model.add(MaxPool2D(pool_size = 2))\n",
    "\n",
    "#fully connected neural network\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dense(3, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28e99ee",
   "metadata": {},
   "source": [
    "## 5. Compile and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c6be7e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "44/44 [==============================] - 11s 234ms/step - loss: 1.0953 - accuracy: 0.4025 - val_loss: 1.0212 - val_accuracy: 0.5160\n",
      "Epoch 2/10\n",
      "44/44 [==============================] - 11s 240ms/step - loss: 0.9886 - accuracy: 0.5117 - val_loss: 0.9644 - val_accuracy: 0.5329\n",
      "Epoch 3/10\n",
      "44/44 [==============================] - 11s 250ms/step - loss: 0.9483 - accuracy: 0.5472 - val_loss: 0.9376 - val_accuracy: 0.5613\n",
      "Epoch 4/10\n",
      "44/44 [==============================] - 11s 258ms/step - loss: 0.9175 - accuracy: 0.5703 - val_loss: 0.9015 - val_accuracy: 0.5772\n",
      "Epoch 5/10\n",
      "44/44 [==============================] - 11s 261ms/step - loss: 0.8723 - accuracy: 0.5906 - val_loss: 0.8355 - val_accuracy: 0.6148\n",
      "Epoch 6/10\n",
      "44/44 [==============================] - 13s 292ms/step - loss: 0.8061 - accuracy: 0.6183 - val_loss: 0.7698 - val_accuracy: 0.6552\n",
      "Epoch 7/10\n",
      "44/44 [==============================] - 13s 295ms/step - loss: 0.7420 - accuracy: 0.6568 - val_loss: 0.7269 - val_accuracy: 0.6471\n",
      "Epoch 8/10\n",
      "44/44 [==============================] - 12s 286ms/step - loss: 0.6867 - accuracy: 0.6892 - val_loss: 0.6655 - val_accuracy: 0.7199\n",
      "Epoch 9/10\n",
      "44/44 [==============================] - 12s 275ms/step - loss: 0.6284 - accuracy: 0.7241 - val_loss: 0.6476 - val_accuracy: 0.6927\n",
      "Epoch 10/10\n",
      "44/44 [==============================] - 12s 285ms/step - loss: 0.5946 - accuracy: 0.7367 - val_loss: 0.5867 - val_accuracy: 0.7461\n"
     ]
    }
   ],
   "source": [
    "#compile model\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "#fit the model on the training and testing sets\n",
    "history = model.fit(x_train, y_train, epochs = 10, batch_size = 512, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4317a0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add early stopping to select best epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d5bf98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5867 - accuracy: 0.7461\n",
      "accuracy of model:  0.7461333274841309\n"
     ]
    }
   ],
   "source": [
    "model_accuracy = model.evaluate(x_test,y_test)\n",
    "print(\"accuracy of model: \", model_accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a198ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shapes_env",
   "language": "python",
   "name": "shapes_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
